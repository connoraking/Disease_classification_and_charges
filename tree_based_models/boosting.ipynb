{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaebb1cf",
   "metadata": {},
   "source": [
    "Source: ISLR\n",
    "\n",
    "In this notebook, I will implement *Boosting* based on decision trees, specifically Gradient Boosting and `XGBoost`. Similar to *Bagging*, *Boosting* is a general approach that can be applied to many machine learning methods. Trees in Boosting are built **sequentially**: each tree is grown using info from previously grown trees. Boosting, notably, does not involve bootstrap sampling and instead is fit on a modified version of the original data set.\n",
    "\n",
    "Boosting learns *slowly*. Given the current model, we fit a decision tree to the residuals from the model instead of $Y$ (hospital charges). When then add this new decision tree into the fitted function in order to update the residuals. By fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it does not perform well. The shrinkage parameter $\\lambda$ slows the process down further, allowing more and different shaped trees to attack the residuals.\n",
    "\n",
    "**Algorithm:**:\n",
    "\n",
    "1. Set $\\hat{f}(x) = 0$ and $r_i = y_i$, $\\forall i $ in the training set \n",
    "2. For $b = 1, 2, \\dots, B$, repeat:\n",
    "    1. Fit a tree $\\hat{f}^b$ with $d$ splits ($d+1$ terminal nodes) to the training data ($X,r$)\n",
    "    2. Update $\\hat{f}$ by adding in a shrunken version of the new tree: $\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)$\n",
    "    3. Update the residuals: $r_i \\leftarrow r_i - \\lambda  \\hat{f}^b(x_i)$\n",
    "    4. Output the boosted model: $\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda  \\hat{f}^b(x)$\n",
    "    \n",
    "Boosting here has three hyperparameters (tuning parameters):\n",
    "1. The number of trees $B$. Unlike *bagging* and *random forests*, boosting can overfit if $B$ is too large. We will use cross-validation to select $B$\n",
    "2. The shrinkage parameter $\\lambda$, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001. Very small $\\lambda$ will require large number of trees $B$\n",
    "3. The number of $d$ splits in each tree which controls the complexity of the boosted ensemble. $d=1$ often wrosk well in which each tree is a *stump*, consisting of a single split. $d$ is also the *interaction depth* and controls the interaction order of the boosted model since $d$ splits can involve at most $d$ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21308d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9584cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a42aa",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c26fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing processed dataframe\n",
    "path_project = \"C:/Users/Conno/Documents/Career/Projects/Hospital_Charges/tree_based_models\"\n",
    "\n",
    "os.chdir(path_project)\n",
    "plots_dir = 'boosting_plots' # stores plots in plot folder\n",
    "\n",
    "df = pd.read_csv(\"../df_processed.csv\")\n",
    "\n",
    "# do this later in data_cleaning file\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb592ca",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b588f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = [\"charges\"])\n",
    "y = df[\"charges\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
